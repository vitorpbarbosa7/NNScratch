{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sizes se refere a o número de neurônios em cada camada, por exemplo 10 neurônios na camada de entrada, 5 neurônios na cada oculta e 1 neurônio na camada de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [10,5,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A partir do número de neurônios em cada camada, gera-se bias aleatórios para as próximas camadas, menos para a camada de entrada\n",
    "- Retorna-se portanto uma lista com vetores colunas com o número de linhas correspondendo ao número de neurônios das camadas ocultas e camada de saída se for o caso "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.56740227],\n",
       "        [-0.6903982 ],\n",
       "        [ 1.59747388],\n",
       "        [-0.32919583],\n",
       "        [-1.04324224]]),\n",
       " array([[-0.40940221]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.random.randn(y, 1) for y in sizes[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geração dos pesos aleatórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (3, 6)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Revisão sobre a função zip\n",
    "x = [1,2,3]\n",
    "y = [4,5,6]\n",
    "list(zip(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Retornar o número de pesos que deverá ser gerado através da função zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 5), (5, 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(sizes[:-1], sizes[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Os pesos aleatórios são gerados da mesma maneira com o bias, ou seja, com a função np.random.randn\n",
    "- Observa-se que é retornado uma lista com dois arrays. \n",
    "- O primeiro array corresponde aos pesos que ligam os neurônios da camada de entrada com a primeira camada oculta \n",
    "- O segundo array corrresponde aos pesos que conectam os neurônios da primeira camada oculta com a camada de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-9.95584639e-01, -1.28647067e+00, -1.00279245e+00,\n",
       "         -9.86279603e-01, -9.30594054e-01,  1.31278636e+00,\n",
       "          1.36674210e+00, -7.71964187e-01,  4.88449401e-01,\n",
       "         -3.64712806e-01],\n",
       "        [ 6.46234377e-01,  5.52698371e-01,  1.09710355e+00,\n",
       "          6.26418785e-01, -1.40184271e-01, -6.85146583e-01,\n",
       "          1.23420769e+00,  7.04152079e-01,  6.47084371e-01,\n",
       "          3.29033006e-01],\n",
       "        [-1.73838955e+00,  2.68687682e+00, -1.26052685e+00,\n",
       "          3.96686410e-01,  1.51464039e+00, -1.22617556e-03,\n",
       "          1.38775556e+00,  4.74477055e-01,  1.81787721e+00,\n",
       "         -3.62233195e-01],\n",
       "        [ 9.91781472e-01,  2.96412351e-02,  4.77657279e-01,\n",
       "         -1.23482647e-01, -1.90344246e-01,  4.54598585e-01,\n",
       "          1.87031774e+00,  6.08605161e-01,  7.03805443e-01,\n",
       "          1.43796753e+00],\n",
       "        [ 1.06780198e+00, -3.48763931e-01,  9.15213793e-01,\n",
       "         -1.36376719e-01, -1.14176901e+00,  3.80734247e-01,\n",
       "         -5.44194159e-01, -8.75200739e-01,  5.85617549e-01,\n",
       "         -1.38693409e+00]]),\n",
       " array([[ 0.68265151, -1.47080116,  1.49699712, -0.00916661,  1.17883126]])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    #sizes do tipo lista\n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        #Passando os atributos da rede neural\n",
    "        #Número de camadas da rede neural\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes [1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pode-se criar um objeto a partir da classe Network e têm-se assim uma arquitetura de rede neural "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Network(sizes=[10,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.num_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 5, 1]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.71268758],\n",
       "        [-0.28836017],\n",
       "        [ 0.07047816],\n",
       "        [ 0.0120389 ],\n",
       "        [ 1.09606926]]),\n",
       " array([[-0.52837378]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.63011454,  0.18660848, -1.63335205,  0.48949745, -1.25851588,\n",
       "         -0.43232617, -0.08534341,  0.3607494 , -0.85260924, -0.31139361],\n",
       "        [ 0.29092307,  0.71216018,  0.48116471, -2.15349388,  0.99175812,\n",
       "         -0.09601449, -1.10323191, -0.16099279,  0.34594325, -0.43996206],\n",
       "        [ 0.62994886, -2.28868755, -0.01086047,  2.02129198,  0.46175618,\n",
       "          1.57588828, -0.76000119, -0.03564152,  0.09346019,  0.40169615],\n",
       "        [-0.4882501 ,  0.44477911, -0.84535334, -0.3517151 , -0.40339094,\n",
       "         -0.71510277, -0.90129742,  2.68919292,  2.22622224, -0.32973476],\n",
       "        [ 1.03817395,  1.35295996,  0.20346065,  0.71744204, -0.65504615,\n",
       "          0.09260409, -1.20844556,  1.35632491,  1.20974323,  0.3435035 ]]),\n",
       " array([[-0.36903966, -0.95083264,  0.5989238 ,  0.24018454,  0.30277209]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Função de ativação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A função de ativação irá ser aplicada sobre: (W*A + b)\n",
    "- Em que:\n",
    "- W: Matriz dos pesos\n",
    "- A: Matriz de entradas\n",
    "- b: Vetor de bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'z' é uma entrada do tipo vetor ou matriz numpy, mas o numpy aplica automaticamente a função sigmoide de forma elementwise, isto é, na for vetorizada "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward\n",
    "- Função feedforward será responsável por dada uma entrada, retornar uma saída, após a aplicação das funções de multiplicação, soma e função de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] [4 5 6] 32\n"
     ]
    }
   ],
   "source": [
    "#Revisãozinha np.dot\n",
    "a1 = np.array([1,2,3])\n",
    "a2 = np.array([4,5,6])\n",
    "adot = np.dot(a1,a2)\n",
    "print(a1,a2,adot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Repetindo o código para fins didático né)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    #sizes do tipo lista\n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        #Passando os atributos da rede neural\n",
    "        #Número de camadas da rede neural\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes [1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    #São realizadas operações sobre a entrada a retorna-se a apóes estas operações\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w,a)+b) #Tá criada em algum lugar do notebook\n",
    "        return a \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualização de como através da função range obtêm-se os mini_batches\n",
    "- Deve-se notar que na função, antes da aplicação da função range, os dados foram embaralhados através da função random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "#Para dar prints horizontais\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 32, 64, 96, 128, 160, 192, 224, 256, 288, 320, 352, 384, 416, 448, 480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800, 832, 864, 896, 928, 960, 992]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,1000,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    #sizes do tipo lista\n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        #Passando os atributos da rede neural\n",
    "        #Número de camadas da rede neural\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes [1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    #São realizadas operações sobre a entrada a retorna-se a apóes estas operações\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w,a)+b) #Tá criada em algum lugar do notebook\n",
    "        return a \n",
    "        \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data = None):\n",
    "        # Este tratamento que retorna o tamanho da base de dados de teste será útil no batch_size\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        #Batch_size, ou seja, conjunto de dados que serão utilizados em capa época para a atualização dos pesos\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            #Retornar o conjunto de dados mini_batches a partir do tamanho do batch passado através da variável mini_batch_size\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            \n",
    "            #eta é a taxa de aprendizagem \n",
    "            for mini_batch in mini_batches:\n",
    "                #Chamando o método update_mini_batch desta classe\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "                \n",
    "            #Se os dados de testes forem passados, será impresso uma linha com a avaliação do modelo nestes dados de teste em cada época\n",
    "            if test_data:\n",
    "                #Chamando o método evaluate desta classe\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} finalizada\".format(j))\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função update_mini_batch\n",
    "- Nabla ($\\nabla$) ou (del) é a letra que representa o gradiente em cálculo vetorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Inicialização dos vetores que receberão os gradientes do bias e gradientes dos pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]]), array([[0.]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gradiente para a atualização dos bias:\n",
    "nabla_b = [np.zeros(b.shape) for b in nn.biases]; nabla_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]), array([[0., 0., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nabla_w = [np.zeros(w.shape) for w in nn.weights]; nabla_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sabemos que mini_batch é uma tupla, porque training_data é um tupla\n",
    "- Esta tupla é constutuída por (x,y), ou seja, os dados de entrada e dados de saída"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antes de mostrar a função update_mini_batch, é interessante mostrar a principal função de todas na atualização dos pesos, ou seja, a self.backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self, x, y):\n",
    "    \n",
    "    #Gradientes dos biases e dos pesos\n",
    "    nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "    \n",
    "    #FeedForward (Desta maneira posso trabalhar com uma única função de ativação em todas camadas? )\n",
    "    activation = x\n",
    "    \n",
    "    #Lista com todas ativações, de camada por camada, para chamar depois\n",
    "    activations = [x]\n",
    "    \n",
    "    #Lista que armazena os vetores z, camada por camada\n",
    "    zs = []\n",
    "    \n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "            \n",
    "        z = np.dot(w, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "    #Backward passa\n",
    "    #Função cost_derivative recebe como parâmetros a função de ativação do nó e a saída do nó (esta seria só a camada de saaída ou todas?)\n",
    "    #Porque y seria a saída não? \n",
    "    #Sigmoid prime é outra função que retorna o valor da derivada no ponto?\n",
    "    #Se estamos trabalhando com [-1], está se referindo aos últimos valores dos vetores, logo camada de saída? \n",
    "    delta = self.cost_derivative(activations[-1],y) * sigmoid_prime(zs[-1])\n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    \n",
    "    #Para cada camada:\n",
    "    for l in range(2, self.num_layers):\n",
    "        z = zs[-l]\n",
    "        sp = sigmoide_prime(z)\n",
    "        \n",
    "        #Este produto que retorna delta a partir da multiplicação com delta é o :\n",
    "        #DeltaCamada1 = pesos*DeltaCamada2 (Camada2 pode ser a camada de saída e Camada 1 a camada oculta, por exemplo)\n",
    "        delta = np.dot(self.weights[-l+1].transpose(), delta)*sp\n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "    #Retorna os dois gradientes\n",
    "    return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora que sabemos como funciona o método BackProp que será uitlizado no método update_mini_batch, podemos voltar para a função principal\n",
    "(Repetindo todo algoritmo por questões didáticas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    #Sizes é um objeto do tipo lista, com o número de camadas da rede\n",
    "    #Aqui definimos os atributos da arquitetura da rede\n",
    "    def __init__(self, sizes):\n",
    "        \n",
    "        #Passando os atributos da rede neural\n",
    "        #Número de camadas da rede neural\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes [1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    #São realizadas operações sobre a entrada a retorna-se 'a' após estas operações\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w,a)+b) #Função sigmoid está presente no final desta classe\n",
    "        return a \n",
    "        \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data = None):\n",
    "        # Este tratamento que retorna o tamanho da base de dados de teste será útil no batch_size\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        #Batch_size, ou seja, conjunto de dados que serão utilizados em capa época para a atualização dos pesos\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            #Retornar o conjunto de dados mini_batches a partir do tamanho do batch passado através da variável mini_batch_size\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0,n,mini_batch_size)]\n",
    "            \n",
    "            #eta é a taxa de aprendizagem \n",
    "            for mini_batch in mini_batches:\n",
    "                #Chamando o método update_mini_batch desta classe\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "                \n",
    "            #Se os dados de testes forem passados, será impresso uma linha com a avaliação do modelo nestes dados de teste em cada época\n",
    "            if test_data:\n",
    "                #Chamando o método evaluate desta classe\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} finalizada\".format(j))\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "            \n",
    "        nabla_b = [np.zteros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "            \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x,y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "        #############################################################################################################\n",
    "        #Atualização dos pesos efetivamente ocorre aqui!\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]            \n",
    "        ##############################################################################################################\n",
    "        \n",
    "                \n",
    "    # O método backprop é responsável por receber todos os valores iniciais dos nós, tanto os nós de entrada e\n",
    "    # os valores de nós aleatórios de saída\n",
    "    # A Função backprop faz uso de outras funções como sigmoid para retornar o valor da aplicação da sigmoid na multiplicação pesos*nós + bias\n",
    "    # Também faz uso da função cost_derivative que traz a diferença entre a saída real e o valor predito? \n",
    "    # ALém de utilizar a função sigmoide_prime que retorna a derivada no nó da função sigmoid \n",
    "    def backprop(self, x, y):\n",
    "    \n",
    "        #Gradientes dos biases e dos pesos\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        #FeedForward (Desta maneira posso trabalhar com uma única função de ativação em todas camadas? )\n",
    "        #Este valor na realidade no início são os valores dos próprios nós não? \n",
    "        #Depois aplica-se a função sigmoide sobre o valor de activation\n",
    "        activation = x\n",
    "\n",
    "        #Lista com todas ativações, de camada por camada, para chamar depois\n",
    "        activations = [x]\n",
    "\n",
    "        #Lista que armazena os vetores z, camada por camada\n",
    "        zs = []\n",
    "        \n",
    "        #Este loop é responsável por retornar todas as ativações, ou seja, todos os valores dos nós\n",
    "        # É efetivamente o Forward Pass, em que a primeira activation é a entrada da rede, os nós de entrada, e ao final\n",
    "        # na última camada do activations têm-se os nós da saída\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "\n",
    "        #Backward pass\n",
    "        #Função cost_derivative recebe como parâmetros a função de ativação do nó e a saída do nó (esta seria só a camada de saaída ou todas?)\n",
    "        #Porque y seria a saída não? \n",
    "        #Sigmoid prime é outra função que retorna o valor da derivada no ponto?\n",
    "        #Se estamos trabalhando com [-1], está se referindo aos últimos valores dos vetores, logo camada de saída? \n",
    "        delta = self.cost_derivative(activations[-1],y) * sigmoid_prime(zs[-1]) #DeltaOutput da saída. (Erro*Derivada)\n",
    "        nabla_b[-1] = delta \n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose()) #DeltaHidden: DeltaOutput*NósDaÚltimaCamadaOculta\n",
    "\n",
    "        #Para cada camada depois de recebidos os gradientes respectivos à última camada\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l] #l-ésimo última camada\n",
    "            sp = sigmoide_prime(z) #Derivada do valor do nó\n",
    "\n",
    "            #Este produto que retorna delta a partir da multiplicação com delta é o :\n",
    "            #DeltaCamada1 = pesos*DeltaCamada2 (Camada2 pode ser a camada de saída e Camada 1 a camada oculta, por exemplo)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta)*sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "\n",
    "        #Retorna os dois gradientes para todos os biases e todos os pesos\n",
    "        return (nabla_b, nabla_w)\n",
    "                \n",
    "    #Avaliar o resultado da rede com dados de teste, se estes forem passados\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)),y) for (x,y) in test_data]\n",
    "        return sum(int(x == y) for (x,y) in test_results)\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def sigmoid(z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "    #Primitiva da sigmoid\n",
    "    def sigmoid_prime(z):\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
